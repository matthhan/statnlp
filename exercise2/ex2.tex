%% Dokumentenklasse (Koma Script) -----------------------------------------
\documentclass[%
   11pt,              % Schriftgroesse
   ngerman,           % wird an andere Pakete weitergereicht
   a4paper,           % Seitengroesse
   DIV11,             % Textbereichsgroesse (siehe Koma Skript Dokumentation !)
]{scrartcl}%     Klassen: scrartcl, scrreprt, scrbook
% -------------------------------------------------------------------------

\usepackage[utf8]{inputenc} % Font Encoding, benoetigt fuer Umlaute
\usepackage[ngerman]{babel}   % Spracheinstellung

\usepackage[T1]{fontenc} % T1 Schrift Encoding
\usepackage{textcomp}    % Zusatzliche Symbole (Text Companion font extension)
\usepackage{lmodern}     % Latin Modern Schrift
\usepackage{listings}
\usepackage{framed}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{framed}
\usepackage{listings}
\usepackage[left=2cm,right=3cm,top=2cm,bottom=2cm,includeheadfoot]{geometry}
%Kopf- und Fußzeile
\usepackage{fancyhdr}

\pagestyle{fancy}
\fancyhf{}
%Übungsteilnehmer
\fancyhead[L]{Matthias Hansen, 331600~~Lukas Huwald, 322890\\}
%Kopfzeile mittig
\fancyhead[R]{NLP Exercise02}
%Linie oben
\renewcommand{\headrulewidth}{0.5pt}

\setlength{\parskip}{1ex}

%Fußzeile links bzw. innen
\fancyfoot[L]{}
%Fußzeile rechts bzw. außen
\fancyfoot[R]{\thepage}
%Linie unten
\renewcommand{\footrulewidth}{0.5pt}
%% Dokument Beginn %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\section*{Task 1}
Bayes' decision rule for some loss function L that depends on some input data x is 

$d(x) = \underset{c}{arg min} \displaystyle\sum_{\tilde{c}} p(\tilde{c}|x) \cdot L[c,\tilde{c}] \stackrel{\text{For these Definitions}}{=}$

$d(x) = \underset{c}{arg min} \displaystyle\sum_{\tilde{c}} p(\tilde{c}|x) \cdot (\gamma - \delta(c,\tilde{c}))$


\subsection*{a)}
We can use the dependency on the selected class:

$d(x) = \underset{c}{arg min} \displaystyle\sum_{\tilde{c}} p(\tilde{c}|x) \cdot (\gamma_c - \delta(c,\tilde{c})) \stackrel{\text{Eliminate Kronecker }\delta}{=}$

$d(x) = \underset{c}{arg min} \displaystyle\sum_{\tilde{c}\neq c} p(\tilde{c}|x) \cdot \gamma_c \stackrel{\text{nothing in the sum depends on c}}{=}$

$d(x) = \underset{c}{arg min} \; \gamma_c\cdot\displaystyle\sum_{\tilde{c}\neq c} p(\tilde{c}|x) \stackrel{\text{probabilities add up to 1}}{=}$

$d(x) = \underset{c}{arg min} \; \gamma_c \cdot (1 - p(c | x))=$

$d(x) = \underset{c}{arg min} \; \gamma_c - \gamma_c \cdot p(c | x)$

\subsection*{b)}
We can use the dependency on the real class by eliminating the Kronecker delta in a different way:

$d(x) = \underset{c}{arg min} \displaystyle\sum_{\tilde{c}} p(\tilde{c}|x) \cdot (\gamma_{\tilde{c}} - \delta(c,\tilde{c})) \stackrel{\text{Eliminate Kronecker }\delta}{=}$

$d(x) = \underset{c}{arg min} (\displaystyle\sum_{\tilde{c}} p(\tilde{c}|x) \cdot \gamma_{\tilde{c}}) - \gamma_{\tilde{c}} \cdot p(c|x) \stackrel{(1)}{=}$

$d(x) = \underset{c}{arg max}\;  \gamma_{\tilde{c}} \cdot p(c|x) \stackrel{\gamma_{\tilde{c}}\text{ is constant w.r.t. c}}{=}$

$d(x) = \underset{c}{arg max}\;  p(c|x)$

Where step (1) is possible, because the sum does not depend on c, i.e. it is constant. Therefore, the second term which is maximal has to be subtracted to get a minimal result. Note especially the shift from argmin to argmax.
\
\section*{Task 2}

To arrive at a maximum likelihood estimate, we have to find:

$\underset{\lambda}{arg max}\; p(x_1^N|\lambda) = $
$\underset{\lambda}{arg max}\;  \displaystyle\prod_{i= 1}^N p(x_i|\lambda) = $
$\underset{\lambda}{arg max}\;  \log(\displaystyle\prod_{i= 1}^N p(x_i|\lambda)) = $
$\underset{\lambda}{arg max}\;  \displaystyle\sum_{i= 1}^N \log(p(x_i|\lambda)) = $
$\underset{\lambda}{arg max}\;  \displaystyle\sum_{i= 1}^N \log(\frac{1}{x_i!}\cdot e^{-\lambda}\cdot \lambda^{x_i} ) = $
$\underset{\lambda}{arg max}\;  \displaystyle\sum_{i= 1}^N (\log(\frac{1}{x_i!}) + \log(e^{-\lambda}) + \log(\lambda^{x_i} )) = $

$\underset{\lambda}{arg max}\;  \displaystyle\sum_{i= 1}^N (\log(\frac{1}{x_i!}) - \lambda + \log(\lambda^{x_i} ))$

We can find the maximizing argument by setting the derivative to zero:

$\frac{\delta}{\delta\lambda} \displaystyle\sum_{i= 1}^N (\log(\frac{1}{x_i!}) - \lambda + \log(\lambda^{x_i} )) = $
$\displaystyle\sum_{i= 1}^N \frac{\delta}{\delta\lambda} (\log(\frac{1}{x_i!}) - \lambda + \log(\lambda^{x_i} )) = $

$\displaystyle\sum_{i= 1}^N \frac{\delta}{\delta\lambda} (\log(\frac{1}{x_i!})) - \frac{\delta}{\delta\lambda}(\lambda) + \frac{\delta}{\delta\lambda}(\log(\lambda^{x_i})) = $
$\displaystyle\sum_{i= 1}^N - 1 + \frac{1}{\lambda^{x_i}} \cdot x_i \cdot \lambda^{x_i-1} = $

$\displaystyle\sum_{i= 1}^N - 1 + \frac{x_i}{\lambda} = $
$-N + \frac{1}{\lambda} \displaystyle\sum_{i= 1}^N   x_i= 0$
$\iff N =\frac{1}{\lambda}\displaystyle\sum_{i= 1}^N  x_i$
$\iff \frac{N}\cdot \lambda =\displaystyle\sum_{i= 1}^N  x_i$
$\iff \lambda =\frac{\sum_{i= 1}^N  x_i}{n}$

So, in the maximum likelihood estimation, $\lambda$ ends up being the average of the observations.
\section*{Task 3}
\subsection*{a)}
We have that 
\begin{equation*}
	p(c, N, N_1^W) = p(c) \cdot p(N|c) \cdot p(N_1^W|N,c)
\end{equation*}
and as mentioned in the task we ignore the $p(N|c)$ here as it should be trained independent of class. We want to train models for $p(c)$ and $p(N_1^W|N,c)$ using maximum likelihood. As discussed in the lecture we model these with independent parameter sets to train them independently. \par
We begin with training $p(c)$. Assume we have given $I$ training examples $(c_i, {N_i}_1^W)$ of count vectors ${N_i}_1^W$ for $W$ different words and class ids $c_i$ for $C$ different classes. As a distribution model we simply choose a finite table, i.e.
\begin{equation}\label{const}
	p(c) = \theta_c \qquad c = 1,\ldots,C
\end{equation} 
and we want to learn parameters $\theta_c$ such that $\sum_{c=1}^C \theta_c = 1$. \\
The likelihood function is 
\begin{equation*}
	L(\theta) = \prod_{i=1}^I p(c_i) = \prod_{i=1}^I \theta_{c_i}
\end{equation*}
and the log likelihood function is
\begin{equation*}
	\hat{L}(\theta) = \sum_{i=1}^I \text{log}(\theta_{c_i})
\end{equation*}
We add the contraint that the $\theta_c$ sum up to unity using lagrangian multiplier $\lambda$:
\begin{equation*}
	\tilde{L}(\theta, \lambda) = \sum_{i=1}^I \text{log}(\theta_{c_i}) - \lambda \cdot (\sum_{c=1}^C \theta_c - 1)
\end{equation*}
We compute the partial derivatives and obtain
\begin{align*}
	\frac{\partial}{\partial \theta_c} \tilde{L}(\theta, \lambda) &= \sum_{i=1}^I \delta(c,c_i) \cdot \frac{1}{\theta_c} - \lambda = \frac{I_c}{\theta_c} - \lambda \\
	\frac{\partial}{\partial \lambda} \tilde{L}(\theta, \lambda) &= \sum_{c=1}^C \theta_c - 1
\end{align*}
where $I_c$ is the number of examples of class $c$. \\
At a maximum, the gradient must vanish, i.e.
\begin{equation}\label{subs}
	\frac{I_c}{\theta_c} - \lambda = 0 \quad \Rightarrow \quad \theta_c = \frac{I_c}{\lambda}
\end{equation}
Substituting this into the constraint (\ref{const}) yields
\begin{equation*}
	\sum_{c=1}^C \frac{I_c}{\lambda} = 1 \quad \Rightarrow \quad \lambda = \sum_{c=1}^C I_c = I
\end{equation*}
Using this value for $\lambda$ in (\ref{subs}) finally yields
\begin{equation*}
	\theta_c = \frac{I_c}{I}
\end{equation*}
so the maximum likelihood estimates are simply the relative frequencies of the classes. \par
To be formally correct, we verify that this is really a maximum by computing the second derivative of the log likelihood function. We obtain for the Hesse matrix
\begin{equation*}
	H_{c,k} := \frac{\partial^2}{\partial \theta_c \partial \theta_k} \hat{L}(\theta) = \begin{cases}
	0 & c \neq k \\
	- \frac{I_c}{\theta_c^2} & c = k \\ \end{cases}
\end{equation*}
So the Hesse matrix is zero except on the diagonal, where it is always negative (the square and $I_c$ are always positive and there is a minus in front). It follows that the Hesse matrix is negative definite at all positions, i.e. the space is concave and the solution must be a global maximum. \par
We continue with training $p(N_1^W|N,c)$ for a given class $c$. Thus, we look at the training data of class $c$ only, i.e. count vectors ${N_i}_1^W$ for $i = 1,\ldots,I_c$. As specified in the lecture we use the multinomial distribution
\begin{equation*}
	p(N_1^W|N,c) = N! \cdot \prod_{w=1}^W \frac{\theta_w^{N_w}}{N_w !}
\end{equation*}
with parameters $\theta_w = p(w|c)$ we want to train, subject to
\begin{equation}\label{cons2}
	\sum_{w=1}^W \theta_w = 1
\end{equation}
The likelihood function is 
\begin{equation*}
	L(\theta) = \prod_{i=1}^{I_c} (\sum_{w=1}^W {N_i}_w)! \cdot \prod_{w=1}^W \frac{\theta_w^{{N_i}_w}}{{N_i}_w !}
\end{equation*}
The term $(\sum_{w=1}^W {N_i}_w)!$ is constant for any $\theta$ and thus we ignore it from now on in the optimization. The same holds for the ${N_i}_w !$ terms.\\
The simplified log likelihood function is
\begin{equation*}
	\hat{L}(\theta) = \sum_{i=1}^{I_c} \sum_{w=1}^W \text{log}(\theta_w) \cdot {N_i}_w
\end{equation*}
We add the contraint that the $\theta_w$ sum up to unity using lagrangian multiplier $\lambda$:
\begin{equation*}
	\tilde{L}(\theta, \lambda) = \sum_{i=1}^{I_c} \sum_{w=1}^W (\text{log}(\theta_w) \cdot {N_i}_w) - \lambda \cdot (\sum_{w=1}^W \theta_w - 1)
\end{equation*}
We compute the partial derivatives and obtain
\begin{align*}
	\frac{\partial}{\partial \theta_w} \tilde{L}(\theta, \lambda) &=  \sum_{i=1}^{I_c} \frac{{N_i}_w}{\theta_w} - \lambda\\
	\frac{\partial}{\partial \lambda} \tilde{L}(\theta, \lambda) &= \sum_{w=1}^W \theta_w - 1
\end{align*}
At a maximum, the gradient must vanish, i.e.
\begin{equation}\label{subs2}
	\sum_{i=1}^{I_c} \frac{{N_i}_w}{\theta_w} - \lambda = 0 \quad \Rightarrow \quad \theta_w = \frac{1}{\lambda} \sum_{i=1}^{I_c} {N_i}_w
\end{equation}
Substituting this into the constraint (\ref{cons2}) yields
\begin{equation*}
	\sum_{w=1}^W \frac{1}{\lambda} \sum_{i=1}^{I_c} {N_i}_w = 1 \quad \Rightarrow \quad \lambda = \sum_{w=1}^W \sum_{i=1}^{I_c} {N_i}_w =: N_c
\end{equation*}
where we denote by $N_c$ the total number of words observed in texts for the class $c$ we consider. Using this value for $\lambda$ in (\ref{subs2}) finally yields
\begin{equation*}
	\theta_w = \frac{1}{N_c} \sum_{i=1}^{I_c} {N_i}_w
\end{equation*}
i.e. the maximum likelihood estimate for the weights $\theta_w = p(w|c)$ are simply the relative frequencies of the words inside the texts of the given class. \par
To be formally correct, we verify that this is really a maximum by computing the second derivative of the log likelihood function. We obtain for the Hesse matrix
\begin{equation*}
	H_{w,v} := \frac{\partial^2}{\partial \theta_w \partial \theta_v} \hat{L}(\theta) = \begin{cases}
	0 & w \neq v \\
	- \frac{1}{\theta_w^2} \sum_{i=1}^{I_c} {N_i}_w & w = v \\ \end{cases}
\end{equation*}
So the Hesse matrix is zero except on the diagonal, where it is always negative (the square and ${N_i}_w$ are always positive and there is a minus in front). It follows that the Hesse matrix is negative definite at all positions, i.e. the space is concave and the solution must be a global maximum.
\subsection*{b)}
We use the same notation for the training data as for a) and start with the likelihood function$(W = 3)$
\begin{equation*}
	L(\theta) = \prod_{i=1}^{I_c} ({N_i}_1 + {N_i}_2 + {N_i}_3)! \cdot \frac{\theta_1^{{N_i}_1}}{{N_i}_1 !} \cdot \frac{\theta_2^{{N_i}_2}}{{N_i}_2 !} \cdot \frac{(1 - \theta_1 - \theta_2)^{{N_i}_3}}{{N_i}_3 !}
\end{equation*}
Again we ignore all the constant factors (the faculty terms). The simplified log likelihood function is
\begin{equation*}
	\hat{L}(\theta) = \sum_{i=1}^{I_c} \text{log}(\theta_1) \cdot {N_i}_1 + \text{log}(\theta_2) \cdot {N_i}_2 + \text{log}(1 - \theta_1 - \theta_2) \cdot {N_i}_3
\end{equation*}
To simplify notation we introduce the following new symbols:
\begin{align}\label{symb}
	N_1 &= \sum_{i=1}^{I_c} {N_i}_1 &\text{total count of word 1 in class}\\
	N_2 &= \sum_{i=1}^{I_c} {N_i}_2 &\text{total count of word 2 in class}\\
	N_3 &= \sum_{i=1}^{I_c} {N_i}_3 &\text{total count of word 3 in class}\\
	N_c &= N_1 + N_2 + N_3 &\text{(as above: total word count in the class)}
\end{align}
The log likelihood function becomes
\begin{equation*}
	\hat{L}(\theta) = \text{log}(\theta_1) \cdot N_1 + \text{log}(\theta_2) \cdot N_2 + \text{log}(1 - \theta_1 - \theta_2) \cdot N_3
\end{equation*}
The partial derivatives are
\begin{align}\label{ins}
	\frac{\partial}{\partial \theta_1} \hat{L}(\theta) &= \frac{1}{\theta_1}N_1 - \frac{1}{1 - \theta_1 - \theta_2} N_3\\
	\frac{\partial}{\partial \theta_2} \hat{L}(\theta) &= \frac{1}{\theta_2}N_2 - \frac{1}{1 - \theta_1 - \theta_2} N_3 
\end{align}
At the maximum the gradient must vanish, i.e. the above two equations must equal 0. Substracting the equations, we obtain
\begin{equation}\label{fin}
	\theta_1 = \frac{N_1}{N_2}\theta_2
\end{equation}
Putting this into (\ref{ins}), we have
\begin{equation*}
	\frac{N_2}{\theta_2} - \frac{1}{1 - (1 + \frac{N_1}{N_2})\theta_2}N_3 = 0
\end{equation*}
Solving for $\theta_2$ yields
\begin{equation*}
	\theta_2 = \frac{N_2}{N_1 + N_2 + N_3} = \frac{N_2}{N_c}
\end{equation*}
Putting this into (\ref{fin}), we get
\begin{equation*}
	\theta_1 =  \frac{N_1}{N_c}
\end{equation*}
and the third parameter is
\begin{equation*}
	1 - \theta_1 - \theta_2 = 1 - \frac{N_1}{N_c} - \frac{N_2}{N_c} = \frac{N_3}{N_c}
\end{equation*}
So by changing the notation back again using (\ref{symb}) we see that we obtained the same result as in a):
\begin{equation*}
	\theta_w = \frac{1}{N_c} \sum_{i=1}^{I_c} {N_i}_w
\end{equation*}
We already verified the second derivative there, so we don't do that again.
\section*{Task 4}
Define the following set:

$w_{N_1^W} = \{w | N_1^W \text{ is the count vector of } w\}$ for any count vector $N_1^W$.

Using probability theory, one can argue that event of the occurence of a count vector is the union of the events of the occurence of each document that has this particular count vector. Due to the axiom of $\sigma$-additivity, we then have $p(N_1^W) =\displaystyle\sum_{w_1^N} p(w_1^N)$. Similarly, we have 
$p(N_1^W|c)= \displaystyle\sum_{w_1^N} p(N_1^W|c) \stackrel{(1)}{=}
\displaystyle\sum_{w_1^N} \displaystyle\prod_{w}^N p(w|c)\stackrel{(2)}{=}
|w_{N_1^W}| \displaystyle\prod_{w}^N p(w|c)\stackrel{(3)}{=}.
\frac{N!}{\prod_w N_w!} \displaystyle\prod_{w}^N p(w|c)$.

Where in (1), we quietly make the assumption that the events of the occurence of each word are independent of each other (which was shown to be a good assumption in the lecture) and in (2), we make use of the fact that the product does not depend on the running variable of the sum and thus the sum can be rewritten as a factor of the size of the set that we sum over. Finally in (3) we use the multinomial coefficient that represents the number of permutations of a set in which certain elements can be repeated, i.e. permutations of a multiset or a count vector.

In the following, we give a short explanation of the multinomial coefficient:
In simple cases, a count vector's entries might only contain 0's and 1's, e.g. each word from the vocabulary occurs at most once. Then the number of words is $\displaystyle\sum_{i=1}^W n_i$ and $|w_{N_1^W}|$ is the number of permutations of this number of words. 

The number of permutations of a set of n elements can be calculated by first considering how many possibilities there are for the first position (n many) and then multiplying with the number of possibilities there are for the second, third, \ldots, n-th position if all previous positions have been filled already (n-1,n-2,\ldots 1). This leads us to the formula $|w_{N_1^W}|= \displaystyle\prod_{i=1}^N i =: N!$ (for this simple case).

However, this approach works only if no word is repeated. Consider e.g. the vocabulary $\{``Alice'',``Bob''\}$ and the count vector $(2,0)$. Treating this count vector as above results in the incorrect answer that there are two distinct permutations. However, there is only one permutation, namely ``Alice Alice'', because two equal words cannot be distinguished in the permutation.

To solve this problem, one can first treat a count vector with repetition as a count vector without repetition and then ``remove'' the duplicate permutations generated this way. The number of duplicate permutations is then the product of the duplicate permutations that are ``caused'' by each word in the dictionary and the number of duplicate permutations caused by one specific word is simply the number of permutations of a set of its count. Thus, one arrives at the formula for permutations with repetition, otherwise known as the multinomial coefficient.


\section*{Task 5}
We define
\begin{equation*}
	\Lambda := \sum_{w=1}^W \lambda_w, \qquad p(w) := \frac{\lambda_w}{\Lambda}
\end{equation*}
Using this, we obtain
\begin{align*}
\prod_{w=1}^W \text{Poisson}(N_w) &= \prod_{w=1}^W \frac{e^{-\lambda_w} \cdot \lambda_w^{N_w}}{N_w !} \\
&= \frac{e^{-\Lambda}}{N!} \cdot N! \prod_{w=1}^W \frac{\lambda_w^{N_w}}{N_w !} \\
&= \frac{e^{-\Lambda} \cdot \Lambda^N}{N!} \cdot N! \prod_{w=1}^W \frac{p(w)^{N_w}}{N_w !} \\
&= \frac{e^{-\Lambda} \cdot \Lambda^N}{N!} \cdot {N \choose N_1,\ldots N_W} \prod_{w=1}^W p(w)^{N_w} \\
&= \text{Poisson}(N) \cdot \text{Multinomial}(N_1^W|N)
\end{align*}
\end{document}
