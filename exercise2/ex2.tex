%% Dokumentenklasse (Koma Script) -----------------------------------------
\documentclass[%
   11pt,              % Schriftgroesse
   ngerman,           % wird an andere Pakete weitergereicht
   a4paper,           % Seitengroesse
   DIV11,             % Textbereichsgroesse (siehe Koma Skript Dokumentation !)
]{scrartcl}%     Klassen: scrartcl, scrreprt, scrbook
% -------------------------------------------------------------------------

\usepackage[utf8]{inputenc} % Font Encoding, benoetigt fuer Umlaute
\usepackage[ngerman]{babel}   % Spracheinstellung

\usepackage[T1]{fontenc} % T1 Schrift Encoding
\usepackage{textcomp}    % Zusatzliche Symbole (Text Companion font extension)
\usepackage{lmodern}     % Latin Modern Schrift
\usepackage{listings}
\usepackage{framed}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{framed}
\usepackage{listings}
\usepackage[left=2cm,right=3cm,top=2cm,bottom=2cm,includeheadfoot]{geometry}
%Kopf- und Fußzeile
\usepackage{fancyhdr}

\pagestyle{fancy}
\fancyhf{}
%Übungsteilnehmer
\fancyhead[L]{Matthias Hansen, 331600~~Lukas Huwald, 322890\\}
%Kopfzeile mittig
\fancyhead[R]{NLP Exercise02}
%Linie oben
\renewcommand{\headrulewidth}{0.5pt}

%Fußzeile links bzw. innen
\fancyfoot[L]{}
%Fußzeile rechts bzw. außen
\fancyfoot[R]{\thepage}
%Linie unten
\renewcommand{\footrulewidth}{0.5pt}
%% Dokument Beginn %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\section*{Task 1}
\section*{Task 2}
\section*{Task 3}
\subsection*{a)}
We have that 
\begin{equation*}
	p(c, N, N_1^W) = p(c) \cdot p(N|c) \cdot p(N_1^W|N,c)
\end{equation*}
and as mentioned in the task we ignore the $p(N|c)$ here as it should be trained independent of class. We want to train models for $p(c)$ and $p(N_1^W|N,c)$ using maximum likelihood. As discussed in the lecture we model these with independent parameter sets to train them independently. \par
We begin with training $p(c)$. Assume we have given $I$ training examples $(c_i, {N_i}_1^W)$ of count vectors ${N_i}_1^W$ for $W$ different words and class ids $c_i$ for $C$ different classes. As a distribution model we simply choose a finite table, i.e.
\begin{equation*}
	p(c) = \theta_c \qquad c = 1,\ldots,C
\end{equation*} 
and we want to learn parameters $\theta_c$ such that $\sum_{c=1}^C \theta_c = 1$. \\
The likelihood function is 
\begin{equation*}
	L(\theta) = \prod_{i=1}^I p(c_i) = \prod_{i=1}^I \theta_{c_i}
\end{equation*}
and the log likelihood function is
\begin{equation*}
	\hat{L}(\theta) = \sum_{i=1}^I \text{log}(\theta_{c_i})
\end{equation*}
We add the contraint that the $\theta$ sum up to unity using lagrangian multiplier $\lambda$:
\begin{equation*}
	\tilde{L}(\theta) = \sum_{i=1}^I \text{log}(\theta_{c_i}) - \lambda \cdot (\sum_{c=1}^C \theta_c - 1)
\end{equation*}
We compute the partial derivatives and obtain
\begin{align*}
	\frac{\partial}{\partial \theta_c} \tilde{L}(\theta) &= \sum_{i=1}^I \delta(c,c_i) \cdot \frac{1}{\theta_c} - \lambda = \frac{N_c}{\theta_c} - \lambda \\
\end{align*}
where $N_c$ is the number of examples of class $c$.

\section*{Task 4}
\section*{Task 5}
\end{document}
